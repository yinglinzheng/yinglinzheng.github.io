(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{313:function(e,n,r){},332:function(e,n,r){"use strict";r(313)},351:function(e,n,r){"use strict";r.r(n);r(332);var t=r(27),i=Object(t.a)({},(function(){var e=this,n=e._self._c;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("ProfileSection",{attrs:{frontmatter:e.$page.frontmatter}}),e._v(" "),n("h2",{attrs:{id:"about-me"}},[e._v("About Me")]),e._v(" "),n("p",[e._v("I'm currently a PhD student in the "),n("a",{attrs:{href:"https://vcg.xmu.edu.cn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Visual Computing and Graphics Lab"),n("OutboundLink")],1),e._v(" of Xiamen University, supervised by professor "),n("a",{attrs:{href:"http://mingzeng.xyz/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Ming Zeng"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[e._v("I had a fantastic experience as a research intern at Visual Computing Group, "),n("a",{attrs:{href:"https://www.msra.cn/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Microsoft Research Asia(MSRA)"),n("OutboundLink")],1),e._v(", working with "),n("a",{attrs:{href:"https://jianminbao.github.io/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Jianmin Bao"),n("OutboundLink")],1),e._v(", "),n("a",{attrs:{href:"https://www.microsoft.com/en-us/research/people/tinzhan/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Ting Zhang"),n("OutboundLink")],1),e._v(", and "),n("a",{attrs:{href:"http://www.dongchen.pro/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dr. Dong Chen"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("p",[e._v("My research interests lie at human-centric computer vision. I serve as the reviewer of CVPR, ICCV, ECCV, AAAI.")]),e._v(" "),n("h2",{attrs:{id:"news"}},[e._v("News")]),e._v(" "),n("ul",[n("li",[e._v("[Jul. 2025] HairShifter has been accepted by ACM MM 2025.")]),e._v(" "),n("li",[e._v("[Apr. 2025] Our work on predicting turn-taking and backchannel in human-machine conversations has been accepted by ACL 2025.")]),e._v(" "),n("li",[e._v("[Feb. 2023] Our work MaskCLIP is accepted by CVPR 2023.")]),e._v(" "),n("li",[e._v("[Dec. 2022] Our work on singing head generation has been accepted by CVM2023 and will be published on CVMJ.")]),e._v(" "),n("li",[e._v("[Nov. 2022] Our work on multi-exposure image fusion named EMEF has been accepted by AAAI2023.")]),e._v(" "),n("li",[e._v("[Oct. 2022] I and my roommate "),n("a",{attrs:{href:"https://wenjindeng.netlify.app/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Wenjin Deng"),n("OutboundLink")],1),e._v(" won the National Scholarship for Postgraduate Students at the same time.")]),e._v(" "),n("li",[e._v("[Sep. 2022] My tutorial about "),n("a",{attrs:{href:"https://arxiv.org/abs/2006.11239",target:"_blank",rel:"noopener noreferrer"}},[e._v("Denoising Diffusion Probabilistic Models"),n("OutboundLink")],1),e._v(" is available on "),n("a",{attrs:{href:"https://www.bilibili.com/video/BV1rW4y1Y7M5/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Bilibili"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[Jun. 2022] The "),n("a",{attrs:{href:"https://github.com/FacePerceiver/LAION-Face",target:"_blank",rel:"noopener noreferrer"}},[e._v("LAION-Face"),n("OutboundLink")],1),e._v(" dataset was released to support large-scale face pretraining.")]),e._v(" "),n("li",[e._v("[Jun. 2022] I received the reward of Excellent from the Star of Tomorrow Internship program in Microsoft Research Asia.")]),e._v(" "),n("li",[e._v("[Apr. 2022] Our work on multi-person pose estimation named "),n("strong",[e._v("I^2R-Net")]),e._v(" is accepted by IJCAI 2022.")]),e._v(" "),n("li",[e._v("[Mar. 2022] "),n("a",{attrs:{href:"https://github.com/FacePerceiver/FaRL",target:"_blank",rel:"noopener noreferrer"}},[e._v("FaRL"),n("OutboundLink")],1),e._v(" is accepted by CVPR 2022 as oral presentation.")]),e._v(" "),n("li",[e._v("[Dec. 2021] Our work on general facial representation learning named "),n("strong",[e._v("FaRL")]),e._v(" is released, code available on "),n("a",{attrs:{href:"https://github.com/FacePerceiver/FaRL",target:"_blank",rel:"noopener noreferrer"}},[e._v("Github"),n("OutboundLink")],1),e._v(".")]),e._v(" "),n("li",[e._v("[Jul. 2021] Our work on Deepfake Detection accepted by ICCV 2021.")])]),e._v(" "),n("h2",{attrs:{id:"publications"}},[e._v("Publications")]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/hairshifter.png"}},[n("p",[e._v("HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation")]),e._v(" "),n("p",[e._v("Wangzheng Shi†, "),n("strong",[e._v("Yinglin Zheng†")]),e._v(", Yuxin Lin, Jianmin Bao, Ming Zeng*, Dong Chen")]),e._v(" "),n("p",[e._v("†Equal contribution")]),e._v(" "),n("p",[n("em",[e._v("ACM MM (CCF-A), 2025")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2507.12758",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/emohuman.png"}},[n("p",[e._v("EmoHuman: Fine-Grained Emotion-Controlled Talking Head Generation via Audio-Text Multimodal Detangling")]),e._v(" "),n("p",[e._v("Qifeng Dai, Huidong Feng, Wendi Cui, Xinqi Cai, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("ICMR (CCF-B), 2025")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://dl.acm.org/doi/10.1145/3731715.3733322",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/consistent-human-animation.png"}},[n("p",[e._v("Consistent Human Animation with Pseudo Multi-View Anchoring and Cross-Granularity Integration")]),e._v(" "),n("p",[e._v("Jintai Wang, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Pengfei Liu, Qifeng Dai , Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("ICMR (CCF-B), 2025")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://dl.acm.org/doi/10.1145/3731715.3733297",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/turn-taking.png"}},[n("p",[e._v("Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals")]),e._v(" "),n("p",[e._v("Yuxin Lin†, "),n("strong",[e._v("Yinglin Zheng†")]),e._v(", Ming Zeng*, Wangzheng Shi")]),e._v(" "),n("p",[e._v("†Equal contribution")]),e._v(" "),n("p",[n("em",[e._v("ACL (CCF-A), 2025")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2505.12654",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/gm2024.png"}},[n("p",[e._v("High-fidelity instructional fashion image editing")]),e._v(" "),n("p",[n("strong",[e._v("Yinglin Zheng")]),e._v(", Ting Zhang, Jianmin Bao, Dong Chen, Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("Graphical Models (CCF-B), 2024")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.sciencedirect.com/science/article/pii/S1524070324000110",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/mmfn.png"}},[n("p",[e._v("Multi-Modal Gait Recognition with Unidirectional Cross-modal Alignment")]),e._v(" "),n("p",[e._v("Hengda Li, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Qifeng Dai, Jintai Wang, Liang Song, Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("ICME (CCF-B), 2024")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://ieeexplore.ieee.org/document/10687404",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/gm2023.png"}},[n("p",[e._v("Vertex position estimation with spatial–temporal transformer for 3D human reconstruction")]),e._v(" "),n("p",[e._v("Xiangjun Zhang, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Wenjin Deng, Qifeng Dai, Yuxin Lin, Wangzheng Shi, Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("Graphical Models (CCF-B), 2023")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.sciencedirect.com/science/article/pii/S1524070323000371",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://vcg.xmu.edu.cn/img/musicface.png"}},[n("p",[e._v("MusicFace: Music-driven Expressive Singing Face Synthesis")]),e._v(" "),n("p",[e._v("Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Yiwei Ding, Xiaohu Guo, Ming Zeng*")]),e._v(" "),n("p",[n("em",[e._v("CVMJ, 2023")])]),e._v(" "),n("p",[n("a",{attrs:{href:"https://link.springer.com/content/pdf/10.1007/s41095-023-0343-7.pdf",target:"_blank",rel:"noopener noreferrer"}},[e._v("PDF"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://s1.ax1x.com/2022/08/29/vfir5D.png"}},[n("p",[e._v("MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining")]),e._v(" "),n("p",[e._v("Xiaoyi Dong, Jianmin Bao, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu")]),e._v(" "),n("p",[e._v("2023, IEEE Conference on Computer Vision and Pattern Recognition(CVPR)")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2208.12262",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/musicface.png"}},[n("p",[e._v("MusicFace: Music-driven Expressive Singing Face Synthesis")]),e._v(" "),n("p",[e._v("Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Yiwei Ding, Xiaohu Guo, Ming Zeng*")]),e._v(" "),n("p",[e._v("2023, Computational Visual Media(CVMJ 2023)")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://vcg.xmu.edu.cn/datasets/singingface/index.html",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dataset"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/emef.png"}},[n("p",[e._v("EMEF: Ensemble Multi-Exposure Image Fusion")]),e._v(" "),n("p",[e._v("Renshuai Liu, Chengyang Li, Haitao Cao, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Ming Zeng, Xuan Cheng*")]),e._v(" "),n("p",[e._v("2023, AAAI Conference on Artificial Intelligence (AAAI), Oral presentation.")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://github.com/medalwill/EMEF",target:"_blank",rel:"noopener noreferrer"}},[e._v("Project"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://s1.ax1x.com/2022/04/21/LyGJKK.png"}},[n("p",[e._v("I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation")]),e._v(" "),n("p",[e._v("Yiwei Ding, Wenjin Deng, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Pengfei Liu, Jianmin Bao, Meihong Wang, Xuan Cheng, Ming Zeng, Dong Chen")]),e._v(" "),n("p",[e._v("2022, The 31st International Joint Conference on Artificial Intelligence (IJCAI-22)")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2206.10892",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v(" "),n("a",{attrs:{href:"https://github.com/leijue222/Intra-and-Inter-Human-Relation-Network-for-MPEE",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"https://s4.ax1x.com/2021/12/08/oRXqhj.png"}},[n("p",[e._v("General Facial Representation Learning in a Visual-Linguistic Manner")]),e._v(" "),n("p",[n("strong",[e._v("Yinglin Zheng")]),e._v(", Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, Fang Wen")]),e._v(" "),n("p",[e._v("2022, IEEE Conference on Computer Vision and Pattern Recognition(CVPR), Oral presentation.")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2112.03109",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v(" "),n("a",{attrs:{href:"https://github.com/FacePerceiver/FaRL",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1),e._v(" "),n("a",{attrs:{href:"https://github.com/FacePerceiver/LAION-Face",target:"_blank",rel:"noopener noreferrer"}},[e._v("Dataset"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/ftcn.png"}},[n("p",[e._v("Exploring Temporal Coherence for More General Video Face Forgery Detection")]),e._v(" "),n("p",[n("strong",[e._v("Yinglin Zheng")]),e._v(", Jianmin Bao, Dong Chen, Ming Zeng, Fang Wen")]),e._v(" "),n("p",[e._v("2021, International Conference on Computer Vision(ICCV)")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://arxiv.org/abs/2108.06693",target:"_blank",rel:"noopener noreferrer"}},[e._v("Paper"),n("OutboundLink")],1),e._v(" "),n("a",{attrs:{href:"https://github.com/yinglinzheng/FTCN",target:"_blank",rel:"noopener noreferrer"}},[e._v("Code"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/mask_face.png"}},[n("p",[e._v("Real-time Masked Face Revealing for Video Conference")]),e._v(" "),n("p",[e._v("Jinpeng Lin, Pengfei Liu, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Wenjin Deng, Ming Zeng")]),e._v(" "),n("p",[e._v("2021, IEEE International Conference on Multimedia and Expo (ICME), Oral presentation.")])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/uv_iccv.png"}},[n("p",[e._v("UV空间中的人脸颜色纹理和几何细节协同补全")]),e._v(" "),n("p",[e._v("程轩，刘仁帅，"),n("strong",[e._v("郑英林")]),e._v("，曾鸣")]),e._v(" "),n("p",[e._v("2020, Chinagraph")])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/vh3d.png"}},[n("p",[e._v("VH3D-LSFM:Video-based Human 3D Pose Estimation with Long-term and Short-term Pose Fusion Mechanism")]),e._v(" "),n("p",[e._v("Wenjin Deng, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Hui Li, Xianwei Wang, Zizhao Wu, Ming Zeng")]),e._v(" "),n("p",[e._v("2020, Chinese Conference on Pattern Recognition and Computer Vision(PRCV)")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.researchgate.net/publication/346167722_VH3D-LSFM_Video-Based_Human_3D_Pose_Estimation_with_Long-Term_and_Short-Term_Pose_Fusion_Mechanism",target:"_blank",rel:"noopener noreferrer"}},[e._v("Link"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/denoise.png"}},[n("p",[e._v("Spatially Adaptive Regularizer for Mesh Denoising")]),e._v(" "),n("p",[e._v("Xuan Cheng, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Yuhui Zheng, Fang Chen, Kunhui Lin")]),e._v(" "),n("p",[e._v("2020, IEEE Access")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.researchgate.net/publication/340572393_Spatially_Adaptive_Regularizer_for_Mesh_Denoising",target:"_blank",rel:"noopener noreferrer"}},[e._v("Link"),n("OutboundLink")],1)])]),e._v(" "),n("ProjectCard",{attrs:{hideBorder:"true",image:"/projects/joint.png"}},[n("p",[e._v("Joint Depth-Face Translation and Facial Alignment via Multi-task Learning")]),e._v(" "),n("p",[e._v("2019, Multimedia Tools and Applications")]),e._v(" "),n("p",[e._v("Xiaoli Wang, "),n("strong",[e._v("Yinglin Zheng")]),e._v(", Ming Zeng, Xuan Cheng, Wei Lu.")]),e._v(" "),n("p",[n("a",{attrs:{href:"https://www.researchgate.net/publication/341411157_Joint_learning_for_face_alignment_and_face_transfer_with_depth_image",target:"_blank",rel:"noopener noreferrer"}},[e._v("Link"),n("OutboundLink")],1)])]),e._v(" "),n("h2",{attrs:{id:"awards-honors"}},[e._v("Awards & Honors")]),e._v(" "),n("ul",[n("li",[e._v("National Scholarship for Graduate Students, 2022")]),e._v(" "),n("li",[e._v("Stars of Tomorrow (Award of Excellent Intern), Microsoft Research Asia, 2022")]),e._v(" "),n("li",[n("strong",[e._v("Rank 10")]),e._v(" among 2265 teams in "),n("strong",[e._v("Kaggle Deepfake Detection Challenge")]),e._v(".")]),e._v(" "),n("li",[n("strong",[e._v("Champion")]),e._v(" of 3D Face Alignment in the Wild Challenge (In conjunction with ICCV 2019), Seoul, Korea, 2019.")])]),e._v(" "),n("h2",{attrs:{id:"education-experiences"}},[e._v("Education & Experiences")]),e._v(" "),n("ul",[n("li",[n("p",[n("strong",[e._v("Research Intern, Visual Computing Group, Microsoft Research Asia")]),e._v(" "),n("br"),e._v("\nDec. 2022 - Mar. 2024")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Research Intern, Visual Computing Group, Microsoft Research Asia")]),e._v(" "),n("br"),e._v("\nSep. 2020 - Mar. 2022")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Doctoral Student of Computer Science, School of Informatics, Xiamen University")]),e._v(" "),n("br"),e._v("\nSep. 2020 - present")])]),e._v(" "),n("li",[n("p",[n("strong",[e._v("Bachelor of Software Engineering, School of Informatics, Xiamen University")]),e._v(" "),n("br"),e._v("\nSep. 2016 - Jun. 2020")])])])],1)}),[],!1,null,null,null);n.default=i.exports}}]);