<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Yinglin Zheng</title>
    <meta name="generator" content="VuePress 1.9.10">
    <link rel="icon" href="/logo.png">
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.2e17252c.css" as="style"><link rel="preload" href="/assets/js/app.ab1bfdca.js" as="script"><link rel="preload" href="/assets/js/2.abd00bac.js" as="script"><link rel="preload" href="/assets/js/1.77ff0ce4.js" as="script"><link rel="preload" href="/assets/js/24.ad7f5287.js" as="script"><link rel="preload" href="/assets/js/20.412c8158.js" as="script"><link rel="preload" href="/assets/js/21.dae4d05f.js" as="script"><link rel="prefetch" href="/assets/js/10.79c5f6bf.js"><link rel="prefetch" href="/assets/js/11.049b9253.js"><link rel="prefetch" href="/assets/js/12.318a0f2c.js"><link rel="prefetch" href="/assets/js/13.720aac23.js"><link rel="prefetch" href="/assets/js/14.acda7471.js"><link rel="prefetch" href="/assets/js/15.fc6a7338.js"><link rel="prefetch" href="/assets/js/16.9598f2e5.js"><link rel="prefetch" href="/assets/js/17.12c7f669.js"><link rel="prefetch" href="/assets/js/18.1198418d.js"><link rel="prefetch" href="/assets/js/19.e780f787.js"><link rel="prefetch" href="/assets/js/22.c8d7328b.js"><link rel="prefetch" href="/assets/js/23.e42a74dd.js"><link rel="prefetch" href="/assets/js/25.d87bf416.js"><link rel="prefetch" href="/assets/js/26.8028842e.js"><link rel="prefetch" href="/assets/js/27.47a9df92.js"><link rel="prefetch" href="/assets/js/28.53bcfe45.js"><link rel="prefetch" href="/assets/js/29.d163c6dd.js"><link rel="prefetch" href="/assets/js/3.da4f016b.js"><link rel="prefetch" href="/assets/js/30.0855d4bf.js"><link rel="prefetch" href="/assets/js/31.aae11b9f.js"><link rel="prefetch" href="/assets/js/32.5abcd86e.js"><link rel="prefetch" href="/assets/js/4.8c932e17.js"><link rel="prefetch" href="/assets/js/5.3123bb7b.js"><link rel="prefetch" href="/assets/js/6.8183c6e5.js"><link rel="prefetch" href="/assets/js/7.fda0aa72.js"><link rel="prefetch" href="/assets/js/vendors~docsearch.3e337192.js">
    <link rel="stylesheet" href="/assets/css/0.styles.2e17252c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" aria-current="page" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Yinglin Zheng</span></a> <div class="links"><!----> <!----></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><!---->  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/profile.jpg" alt></div> <div class="info"><div class="name">
      Yinglin Zheng（郑英林）
    </div> <div class="bio"><p>Student at Xiamen University</p></div> <div class="socials"><div><a href="https://github.com/yinglinzheng" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div><div><a href="https://www.linkedin.com/in/%E8%8B%B1%E6%9E%97-%E9%83%91-b17715135/" target="_blank"><img src="/icons/linkedin-mono.svg" alt="linkedin" title="linkedin"></a></div></div> <div class="contact"><div title="Contact me" class="email">zhengyinglin@stu.xmu.edu.cn</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>I'm currently a PhD student in the <a href="https://vcg.xmu.edu.cn/" target="_blank" rel="noopener noreferrer">Visual Computing and Graphics Lab<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> of Xiamen University, supervised by professor <a href="http://mingzeng.xyz/" target="_blank" rel="noopener noreferrer">Ming Zeng<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>I had a fantastic experience as a research intern at Visual Computing Group, <a href="https://www.msra.cn/" target="_blank" rel="noopener noreferrer">Microsoft Research Asia(MSRA)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, working with <a href="https://jianminbao.github.io/" target="_blank" rel="noopener noreferrer">Dr. Jianmin Bao<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, <a href="https://www.microsoft.com/en-us/research/people/tinzhan/" target="_blank" rel="noopener noreferrer">Dr. Ting Zhang<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>, and <a href="http://www.dongchen.pro/" target="_blank" rel="noopener noreferrer">Dr. Dong Chen<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</p> <p>My research interests lie at human-centric computer vision. I serve as the reviewer of CVPR, ICCV, ECCV, AAAI.</p> <h2 id="news">News</h2> <ul><li>[Jul. 2025] HairShifter has been accepted by ACM MM 2025.</li> <li>[Apr. 2025] Our work on predicting turn-taking and backchannel in human-machine conversations has been accepted by ACL 2025.</li> <li>[Feb. 2023] Our work MaskCLIP is accepted by CVPR 2023.</li> <li>[Dec. 2022] Our work on singing head generation has been accepted by CVM2023 and will be published on CVMJ.</li> <li>[Nov. 2022] Our work on multi-exposure image fusion named EMEF has been accepted by AAAI2023.</li> <li>[Oct. 2022] I and my roommate <a href="https://wenjindeng.netlify.app/" target="_blank" rel="noopener noreferrer">Wenjin Deng<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> won the National Scholarship for Postgraduate Students at the same time.</li> <li>[Sep. 2022] My tutorial about <a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer">Denoising Diffusion Probabilistic Models<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is available on <a href="https://www.bilibili.com/video/BV1rW4y1Y7M5/" target="_blank" rel="noopener noreferrer">Bilibili<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[Jun. 2022] The <a href="https://github.com/FacePerceiver/LAION-Face" target="_blank" rel="noopener noreferrer">LAION-Face<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> dataset was released to support large-scale face pretraining.</li> <li>[Jun. 2022] I received the reward of Excellent from the Star of Tomorrow Internship program in Microsoft Research Asia.</li> <li>[Apr. 2022] Our work on multi-person pose estimation named <strong>I^2R-Net</strong> is accepted by IJCAI 2022.</li> <li>[Mar. 2022] <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">FaRL<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> is accepted by CVPR 2022 as oral presentation.</li> <li>[Dec. 2021] Our work on general facial representation learning named <strong>FaRL</strong> is released, code available on <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">Github<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a>.</li> <li>[Jul. 2021] Our work on Deepfake Detection accepted by ICCV 2021.</li></ul> <h2 id="publications">Publications</h2> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/hairshifter.png" alt></div> <div class="card-content"><p>HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation</p> <p>Wangzheng Shi†, <strong>Yinglin Zheng†</strong>, Yuxin Lin, Jianmin Bao, Ming Zeng*, Dong Chen</p> <p>†Equal contribution</p> <p><em>ACM MM (CCF-A), 2025</em></p> <p><a href="https://arxiv.org/abs/2507.12758" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/emohuman.png" alt></div> <div class="card-content"><p>EmoHuman: Fine-Grained Emotion-Controlled Talking Head Generation via Audio-Text Multimodal Detangling</p> <p>Qifeng Dai, Huidong Feng, Wendi Cui, Xinqi Cai, <strong>Yinglin Zheng</strong>, Ming Zeng*</p> <p><em>ICMR (CCF-B), 2025</em></p> <p><a href="https://dl.acm.org/doi/10.1145/3731715.3733322" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/consistent-human-animation.png" alt></div> <div class="card-content"><p>Consistent Human Animation with Pseudo Multi-View Anchoring and Cross-Granularity Integration</p> <p>Jintai Wang, <strong>Yinglin Zheng</strong>, Pengfei Liu, Qifeng Dai , Ming Zeng*</p> <p><em>ICMR (CCF-B), 2025</em></p> <p><a href="https://dl.acm.org/doi/10.1145/3731715.3733297" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/turn-taking.png" alt></div> <div class="card-content"><p>Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals</p> <p>Yuxin Lin†, <strong>Yinglin Zheng†</strong>, Ming Zeng*, Wangzheng Shi</p> <p>†Equal contribution</p> <p><em>ACL (CCF-A), 2025</em></p> <p><a href="https://arxiv.org/abs/2505.12654" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/gm2024.png" alt></div> <div class="card-content"><p>High-fidelity instructional fashion image editing</p> <p><strong>Yinglin Zheng</strong>, Ting Zhang, Jianmin Bao, Dong Chen, Ming Zeng*</p> <p><em>Graphical Models (CCF-B), 2024</em></p> <p><a href="https://www.sciencedirect.com/science/article/pii/S1524070324000110" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/mmfn.png" alt></div> <div class="card-content"><p>Multi-Modal Gait Recognition with Unidirectional Cross-modal Alignment</p> <p>Hengda Li, <strong>Yinglin Zheng</strong>, Qifeng Dai, Jintai Wang, Liang Song, Ming Zeng*</p> <p><em>ICME (CCF-B), 2024</em></p> <p><a href="https://ieeexplore.ieee.org/document/10687404" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/gm2023.png" alt></div> <div class="card-content"><p>Vertex position estimation with spatial–temporal transformer for 3D human reconstruction</p> <p>Xiangjun Zhang, <strong>Yinglin Zheng</strong>, Wenjin Deng, Qifeng Dai, Yuxin Lin, Wangzheng Shi, Ming Zeng*</p> <p><em>Graphical Models (CCF-B), 2023</em></p> <p><a href="https://www.sciencedirect.com/science/article/pii/S1524070323000371" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://vcg.xmu.edu.cn/img/musicface.png" alt></div> <div class="card-content"><p>MusicFace: Music-driven Expressive Singing Face Synthesis</p> <p>Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, <strong>Yinglin Zheng</strong>, Yiwei Ding, Xiaohu Guo, Ming Zeng*</p> <p><em>CVMJ, 2023</em></p> <p><a href="https://link.springer.com/content/pdf/10.1007/s41095-023-0343-7.pdf" target="_blank" rel="noopener noreferrer">PDF<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://s1.ax1x.com/2022/08/29/vfir5D.png" alt></div> <div class="card-content"><p>MaskCLIP: Masked Self-Distillation Advances Contrastive Language-Image Pretraining</p> <p>Xiaoyi Dong, Jianmin Bao, <strong>Yinglin Zheng</strong>, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu</p> <p>2023, IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</p> <p><a href="https://arxiv.org/abs/2208.12262" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/musicface.png" alt></div> <div class="card-content"><p>MusicFace: Music-driven Expressive Singing Face Synthesis</p> <p>Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, <strong>Yinglin Zheng</strong>, Yiwei Ding, Xiaohu Guo, Ming Zeng*</p> <p>2023, Computational Visual Media(CVMJ 2023)</p> <p><a href="https://vcg.xmu.edu.cn/datasets/singingface/index.html" target="_blank" rel="noopener noreferrer">Dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/emef.png" alt></div> <div class="card-content"><p>EMEF: Ensemble Multi-Exposure Image Fusion</p> <p>Renshuai Liu, Chengyang Li, Haitao Cao, <strong>Yinglin Zheng</strong>, Ming Zeng, Xuan Cheng*</p> <p>2023, AAAI Conference on Artificial Intelligence (AAAI), Oral presentation.</p> <p><a href="https://github.com/medalwill/EMEF" target="_blank" rel="noopener noreferrer">Project<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://s1.ax1x.com/2022/04/21/LyGJKK.png" alt></div> <div class="card-content"><p>I^2R-Net: Intra- and Inter-Human Relation Network for Multi-Person Pose Estimation</p> <p>Yiwei Ding, Wenjin Deng, <strong>Yinglin Zheng</strong>, Pengfei Liu, Jianmin Bao, Meihong Wang, Xuan Cheng, Ming Zeng, Dong Chen</p> <p>2022, The 31st International Joint Conference on Artificial Intelligence (IJCAI-22)</p> <p><a href="https://arxiv.org/abs/2206.10892" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/leijue222/Intra-and-Inter-Human-Relation-Network-for-MPEE" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="https://s4.ax1x.com/2021/12/08/oRXqhj.png" alt></div> <div class="card-content"><p>General Facial Representation Learning in a Visual-Linguistic Manner</p> <p><strong>Yinglin Zheng</strong>, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, Fang Wen</p> <p>2022, IEEE Conference on Computer Vision and Pattern Recognition(CVPR), Oral presentation.</p> <p><a href="https://arxiv.org/abs/2112.03109" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/FacePerceiver/FaRL" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/FacePerceiver/LAION-Face" target="_blank" rel="noopener noreferrer">Dataset<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/ftcn.png" alt></div> <div class="card-content"><p>Exploring Temporal Coherence for More General Video Face Forgery Detection</p> <p><strong>Yinglin Zheng</strong>, Jianmin Bao, Dong Chen, Ming Zeng, Fang Wen</p> <p>2021, International Conference on Computer Vision(ICCV)</p> <p><a href="https://arxiv.org/abs/2108.06693" target="_blank" rel="noopener noreferrer">Paper<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> <a href="https://github.com/yinglinzheng/FTCN" target="_blank" rel="noopener noreferrer">Code<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/mask_face.png" alt></div> <div class="card-content"><p>Real-time Masked Face Revealing for Video Conference</p> <p>Jinpeng Lin, Pengfei Liu, <strong>Yinglin Zheng</strong>, Wenjin Deng, Ming Zeng</p> <p>2021, IEEE International Conference on Multimedia and Expo (ICME), Oral presentation.</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/uv_iccv.png" alt></div> <div class="card-content"><p>UV空间中的人脸颜色纹理和几何细节协同补全</p> <p>程轩，刘仁帅，<strong>郑英林</strong>，曾鸣</p> <p>2020, Chinagraph</p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/vh3d.png" alt></div> <div class="card-content"><p>VH3D-LSFM:Video-based Human 3D Pose Estimation with Long-term and Short-term Pose Fusion Mechanism</p> <p>Wenjin Deng, <strong>Yinglin Zheng</strong>, Hui Li, Xianwei Wang, Zizhao Wu, Ming Zeng</p> <p>2020, Chinese Conference on Pattern Recognition and Computer Vision(PRCV)</p> <p><a href="https://www.researchgate.net/publication/346167722_VH3D-LSFM_Video-Based_Human_3D_Pose_Estimation_with_Long-Term_and_Short-Term_Pose_Fusion_Mechanism" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/denoise.png" alt></div> <div class="card-content"><p>Spatially Adaptive Regularizer for Mesh Denoising</p> <p>Xuan Cheng, <strong>Yinglin Zheng</strong>, Yuhui Zheng, Fang Chen, Kunhui Lin</p> <p>2020, IEEE Access</p> <p><a href="https://www.researchgate.net/publication/340572393_Spatially_Adaptive_Regularizer_for_Mesh_Denoising" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <div class="md-card"><div class="card-image"><img src="/projects/joint.png" alt></div> <div class="card-content"><p>Joint Depth-Face Translation and Facial Alignment via Multi-task Learning</p> <p>2019, Multimedia Tools and Applications</p> <p>Xiaoli Wang, <strong>Yinglin Zheng</strong>, Ming Zeng, Xuan Cheng, Wei Lu.</p> <p><a href="https://www.researchgate.net/publication/341411157_Joint_learning_for_face_alignment_and_face_transfer_with_depth_image" target="_blank" rel="noopener noreferrer">Link<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p></div></div> <h2 id="awards-honors">Awards &amp; Honors</h2> <ul><li>National Scholarship for Graduate Students, 2022</li> <li>Stars of Tomorrow (Award of Excellent Intern), Microsoft Research Asia, 2022</li> <li><strong>Rank 10</strong> among 2265 teams in <strong>Kaggle Deepfake Detection Challenge</strong>.</li> <li><strong>Champion</strong> of 3D Face Alignment in the Wild Challenge (In conjunction with ICCV 2019), Seoul, Korea, 2019.</li></ul> <h2 id="education-experiences">Education &amp; Experiences</h2> <ul><li><p><strong>Research Intern, Visual Computing Group, Microsoft Research Asia</strong> <br>
Dec. 2022 - Mar. 2024</p></li> <li><p><strong>Research Intern, Visual Computing Group, Microsoft Research Asia</strong> <br>
Sep. 2020 - Mar. 2022</p></li> <li><p><strong>Doctoral Student of Computer Science, School of Informatics, Xiamen University</strong> <br>
Sep. 2020 - present</p></li> <li><p><strong>Bachelor of Software Engineering, School of Informatics, Xiamen University</strong> <br>
Sep. 2016 - Jun. 2020</p></li></ul></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">7/24/2025, 3:37:39 AM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.ab1bfdca.js" defer></script><script src="/assets/js/2.abd00bac.js" defer></script><script src="/assets/js/1.77ff0ce4.js" defer></script><script src="/assets/js/24.ad7f5287.js" defer></script><script src="/assets/js/20.412c8158.js" defer></script><script src="/assets/js/21.dae4d05f.js" defer></script>
  </body>
</html>
